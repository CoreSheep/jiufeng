[{"categories":["AI Research"],"content":"The Great AI Divide: ChatGPT vs Claude Usage Patterns Reveal Distinct User Preferences Introduction In an unprecedented move, both OpenAI and Anthropic have released comprehensive usage studies of their flagship AI assistants, ChatGPT and Claude. These dueling analyses provide fascinating insights into how different AI models are carving out distinct niches in the rapidly evolving landscape of artificial intelligence.\nThe findings reveal a striking pattern: rather than competing head-to-head, ChatGPT and Claude are becoming specialized tools for different types of users and use cases. This market segmentation suggests a maturing AI ecosystem where various models excel in specific domains.\nTale of Two AI Assistants ChatGPT: The Personal Companion OpenAI’s analysis of ChatGPT usage reveals a platform that has evolved into a personal AI companion:\nUsage Breakdown: 70%+ non-work related conversations (up from 53% in June 2024) 27% work-related queries (down from 47% year-over-year) Focus on consumer applications over enterprise tools Primary Use Cases: Practical guidance and life advice Creative writing and content assistance Information seeking and general knowledge Casual conversation and exploration Claude: The Professional Powerhouse Anthropic’s research on Claude usage paints a very different picture - one of a work-focused productivity tool:\nUsage Characteristics: Heavy emphasis on professional productivity Coding dominates with 36% of total usage (vs. ChatGPT’s 4.2%) Business automation and task delegation Research and education applications Primary Applications: Software development and programming Scientific research and analysis Business automation and workflows Educational content and tutoring The Great Coding Divide Perhaps the most striking difference between the platforms lies in programming and technical usage:\nClaude: Developer’s Choice 36% of Claude usage involves coding and programming 44% of API usage dedicated to software development Preferred by software engineers for complex technical tasks Strong performance in code generation and debugging ChatGPT: Limited Technical Appeal Only 4.2% programming-related messages Lowest user satisfaction in technical help categories Preference for non-technical applications General-purpose rather than specialized technical tool This disparity suggests that developers have made a clear choice: Claude for coding, ChatGPT for everything else.\nBusiness vs. Consumer Focus Claude: Enterprise-First Approach Claude shows strong business adoption patterns:\nAPI Usage Dominance: 77% of API tasks are automated (vs. 50% on Claude.ai) Full task delegation preferred over collaboration Business process automation as primary use case Administrative support and workflow optimization Professional Applications: Mathematical tasks and data analysis Scientific research (7% of usage, growing) Educational content development (13% and growing) Business operations and management ChatGPT: Consumer-Centric Evolution ChatGPT demonstrates clear consumer market focus:\nPersonal Applications: Life advice and decision support Creative projects and hobbies Learning and skill development Entertainment and casual interaction Work Usage Patterns: Advisory role preferred over task execution Decision support rather than automation Writing assistance remains primary professional use Individual productivity over business process automation Market Segmentation Analysis The usage patterns reveal distinct market segments:\nClaude’s Core Demographics User Type Primary Use Cases Value Proposition Software Developers Code generation, debugging, technical documentation Specialized programming expertise Researchers Data analysis, research assistance, scientific writing Advanced reasoning capabilities Business Professionals Process automation, administrative tasks Efficient task delegation Educators Curriculum development, educational content Structured knowledge delivery ChatGPT’s User Base User Type Primary Use Cases Value Proposition General Consumers Life advice, entertainment, learning Accessible AI companion Creative Professionals Writing, brainstorming, content creation Creative collaboration partner Knowledge Workers Writing assistance, research support Personal productivity enhancement Students Homework help, concept explanation Personalized tutoring Platform Architecture Implications Claude: Built for Business Claude’s usage patterns reflect platform design optimized for:\nTechnical Sophistication: Advanced reasoning capabilities for complex problems Structured output for business applications API-first approach for integration Batch processing and automation support Professional Features: Higher context limits for large documents Better code understanding and generation Scientific and mathematical reasoning Process automation capabilities ChatGPT: Designed for Accessibility ChatGPT’s evolution toward consumer use reflects:\nUser Experience Focus: Conversational fluency over technical precision Broad accessibility across skill levels Intuitive interaction patterns Personal relevance and engagement Consumer-Friendly Features: Multiple interaction modes (text, voice, image) Plugin ecosystem for extended functionality Easy onboarding and adoption Personalization and memory features The Complementary AI Ecosystem Rather than direct competition, the data suggests complementary positioning:\nWorkflow Specialization Many users likely employ both platforms for different purposes:\nDevelopment Workflow: Claude for coding and technical implementation ChatGPT for planning and conceptual work Claude for debugging and optimization ChatGPT for documentation and communication Research Workflow: Claude for data analysis and technical research ChatGPT for brainstorming and ideation Claude for structured reporting ChatGPT for presentation and communication Market Evolution The differentiation suggests mature market development:\nSpecialized Solutions: Different AI models for different needs Platform-specific optimization and features User base segmentation by use case Reduced direct competition through differentiation Future Implications Platform Strategy The usage patterns suggest different strategic directions:\nClaude’s Path: Deeper enterprise integration Advanced technical capabilities Professional workflow optimization B2B market expansion ChatGPT’s Direction: Broader consumer adoption Personal AI companion features Lifestyle integration B2C market dominance User Experience Evolution Different platforms will likely optimize for different experiences:\nProfessional AI (Claude): Efficiency and accuracy prioritized Structured interactions and outputs Integration with business tools Measurable productivity gains Personal AI (ChatGPT): Engagement and accessibility prioritized Natural conversation and interaction Personal context and memory Emotional connection and support Challenges and Opportunities For Claude (Anthropic) Opportunities: Enterprise market expansion Developer tool integration Scientific research partnerships Professional certification programs Challenges: Consumer market penetration Accessibility for non-technical users Price sensitivity in broader markets Brand recognition vs. OpenAI For ChatGPT (OpenAI) Opportunities: Mass market adoption Consumer product integration Educational partnerships Entertainment and media applications Challenges: Professional tool credibility Technical user acquisition Enterprise feature gaps Developer ecosystem competition Conclusion: A Multi-AI Future The dueling studies reveal a crucial insight: the AI market is big enough for specialized players. Rather than a winner-take-all scenario, we’re seeing the emergence of a multi-AI ecosystem where different models serve different needs.\nKey Takeaways: Specialization over generalization: Models are finding specific niches User preference drives adoption: Different users have different needs Complementary rather than competitive: Platforms serve different purposes Market maturation: Clear segmentation suggests industry growth Multiple AI future: Users will likely use multiple AI tools Strategic Implications: Platform differentiation becomes crucial for success User experience optimization for specific use cases Partnership opportunities between complementary platforms Market education about appropriate tool selection Investment in specialized capabilities rather than general features The data tells a clear story: the future of AI is not about one model ruling them all, but about the right AI for the right job. As the technology matures, we can expect even more specialized AI tools to emerge, each optimized for specific use cases and user needs.\nThis market evolution benefits everyone - users get better tools for their specific needs, and AI companies can focus on what they do best rather than trying to be everything to everyone.\nSources: OpenAI ChatGPT usage analysis and Anthropic Claude usage research, as reported by Fortune and official company publications.\n","description":"Dueling studies from OpenAI and Anthropic reveal fascinating differences in how users interact with ChatGPT and Claude, suggesting complementary rather than competing AI ecosystems.","tags":["chatgpt","claude","ai-comparison","usage-patterns","anthropic","openai"],"title":"The Great AI Divide: ChatGPT vs Claude Usage Patterns Reveal Distinct User Preferences","uri":"/blogs/2025/sep/chatgpt-vs-claude-usage-patterns/"},{"categories":["AI Research"],"content":"How People Are Using ChatGPT: Insights from OpenAI’s Latest Usage Analysis Introduction Understanding how people actually use artificial intelligence tools provides crucial insights into the technology’s real-world impact and future trajectory. OpenAI recently released a comprehensive analysis of ChatGPT usage patterns, examining millions of conversations to understand how people interact with their flagship AI assistant.\nThe findings reveal a surprising trend: ChatGPT is increasingly becoming a personal companion and exploratory tool rather than primarily a workplace productivity enhancer. This shift has significant implications for how we think about AI adoption and the future of human-AI interaction.\nKey Findings: The Personal AI Revolution Non-Work Usage Dominates The most striking finding from OpenAI’s analysis is the overwhelming dominance of personal, non-work-related usage:\n70%+ of all ChatGPT conversations are now non-work related (up from 53% in June 2024) Only 27% of messages are work-related (down from 47% a year ago) This represents a fundamental shift in how people perceive and use AI tools This trend suggests that ChatGPT has successfully crossed the chasm from being a niche productivity tool to becoming a mainstream consumer application that people integrate into their daily lives.\nThe Big Three: Core Use Cases OpenAI identified three dominant conversation categories that collectively account for nearly 78% of all ChatGPT interactions:\n1. Practical Guidance (Personal Advisory) Users frequently turn to ChatGPT for:\nLife advice and decision-making support Problem-solving assistance Learning new skills or concepts Getting explanations for complex topics 2. Writing Assistance ChatGPT serves as a versatile writing companion for:\nCreative writing projects Email composition and editing Content brainstorming Personal correspondence improvement 3. Information Seeking People use ChatGPT as an intelligent search engine for:\nQuick fact-checking Research on various topics Summarizing complex information Exploring new subjects Work-Related Usage: Quality Over Quantity While work usage represents a smaller percentage, the quality and nature of professional ChatGPT use reveals interesting patterns:\nAdvisory Role Preferred When people do use ChatGPT for work, they prefer it as:\nDecision support tool rather than task executor Research assistant for gathering and analyzing information Brainstorming partner for idea generation Strategic advisor for planning and problem-solving This preference for advisory roles over direct task execution suggests users maintain agency while leveraging AI for enhanced decision-making.\nWriting Dominates Professional Use 42% of all work-related messages involve writing tasks, including:\nDocument editing and refinement (most common) Email composition and improvement Report writing and formatting Content creation and optimization Interestingly, two-thirds of writing requests involve modifying existing text rather than creating original content from scratch, indicating ChatGPT’s strength in iterative improvement.\nProfessional Demographics The data reveals clear demographic patterns in work usage:\nHighly-paid professionals and technical workers are more likely to use ChatGPT for work Management and business occupations rely heavily on writing assistance Technical professionals use ChatGPT for problem-solving and research The Coding Conundrum One surprising finding challenges common assumptions about AI coding assistance:\nLimited Programming Usage Only 4.2% of total ChatGPT messages relate to computer programming This is significantly lower than competing platforms like Claude (36% for coding) Technical Help category (including programming) shows the lowest user satisfaction Why the Disconnect? Several factors may explain this pattern:\nSpecialized tools preferred: Developers may prefer dedicated coding assistants Context limitations: ChatGPT’s context window may be insufficient for complex coding tasks Integration challenges: Lack of seamless IDE integration compared to specialized tools User expectations: Developers may have higher standards for code quality and accuracy Implications for AI Development The Consumer AI Market OpenAI’s findings suggest a massive consumer AI market that extends far beyond workplace productivity:\nPersonal AI Companions People want AI that understands their personal context Emotional support and life guidance represent significant use cases The boundary between utility and companionship continues to blur Educational Applications Self-directed learning through conversational AI Personalized explanation and tutoring Skill development outside formal educational settings User Experience Priorities The data indicates users value:\nConversational fluency over technical precision Broad knowledge over deep specialization Accessibility over advanced features Personal relevance over professional optimization The Future of Human-AI Interaction Emerging Patterns Several trends emerge from the usage data:\n1. AI as Thinking Partner Users increasingly treat ChatGPT as a cognitive extension rather than a tool:\nBouncing ideas and getting feedback Working through complex problems collaboratively Using AI to explore different perspectives 2. Democratization of Expertise ChatGPT enables users to access expert-level knowledge across domains:\nLegal and medical information (with appropriate disclaimers) Technical explanations made accessible Professional advice and best practices 3. Creative Collaboration The platform facilitates new forms of human-AI creative partnership:\nCollaborative storytelling and content creation Idea generation and refinement Creative problem-solving approaches Market Segmentation The usage patterns suggest clear market segmentation:\nSegment Primary Use Cases Key Value Proposition Personal Users Life advice, learning, entertainment Accessible AI companion Professional Knowledge Workers Writing, research, decision support Productivity enhancement Creative Professionals Content creation, brainstorming Creative partnership Students/Learners Education, skill development Personalized tutoring Challenges and Considerations Quality vs. Accessibility Trade-offs The shift toward personal usage raises important questions:\nAccuracy Concerns Personal advice lacks professional accountability Medical, legal, and financial guidance require careful disclaimers Risk of over-reliance on AI for critical decisions Educational Impact Potential dependency on AI for learning Questions about developing independent thinking skills Balance between AI assistance and human capability development Privacy and Data Sensitivity Personal usage patterns create new privacy considerations:\nIntimate conversations require robust data protection Personal information security becomes paramount Trust becomes the foundation of the user relationship Conclusion: The Personal AI Era OpenAI’s usage analysis reveals a fundamental shift in AI adoption: we’re entering the era of personal AI. Rather than being confined to workplace productivity, ChatGPT has become a digital companion that people turn to for a wide range of personal needs.\nKey Takeaways Personal use dominates: 70%+ of ChatGPT usage is non-work related Advisory preferred: Users want decision support, not task replacement Writing is king: Text creation and editing remain the killer application Accessibility matters: Broad usability trumps specialized features Consumer market massive: Personal AI represents a larger opportunity than enterprise AI Looking Forward This trend suggests several important developments:\nAI will become more conversational and personal Privacy and trust will become key differentiators Educational applications will expand significantly New forms of human-AI collaboration will emerge The definition of “productivity” will expand beyond work As AI technology continues to evolve, understanding these usage patterns helps us build better tools that truly serve human needs. The future of AI may be less about replacing human capabilities and more about augmenting human potential in deeply personal ways.\nThe data tells a clear story: AI is becoming personal, and that’s exactly what users want.\nSource: OpenAI’s analysis of ChatGPT usage patterns, examining millions of user conversations to understand real-world AI interaction patterns.\n","description":"OpenAI reveals fascinating insights into how millions of users interact with ChatGPT, showing a shift toward personal and exploratory use cases rather than work-focused applications.","tags":["chatgpt","openai","ai-usage","productivity","consumer-ai"],"title":"How People Are Using ChatGPT: Insights from OpenAI's Latest Usage Analysis","uri":"/blogs/2025/sep/how-people-use-chatgpt/"},{"categories":["AI Research"],"content":"Defeating Nondeterminism in LLM Inference: A Breakthrough in Deep Learning System Reproducibility Introduction Reproducibility stands as one of the fundamental pillars of scientific research. However, when dealing with large language models (LLMs), achieving reproducible results has proven extraordinarily challenging. Even when we set the temperature parameter to 0 (greedy sampling), which should theoretically produce deterministic results, LLM inference still exhibits nondeterministic behavior.\nRecently, the Thinking Machines team published groundbreaking research that deeply investigates the root causes of nondeterminism in LLM inference and proposes effective solutions. This article provides an in-depth analysis of the core principles and methods of this research.\nThe Nature of the Problem: Floating-Point Non-Associativity The “Original Sin” of Floating-Point Operations To understand the root of nondeterminism, we must first grasp the concept of floating-point non-associativity. In floating-point arithmetic:\n$$(a + b) + c \\neq a + (b + c)$$\nThis seemingly simple mathematical property is actually the fundamental cause of nondeterminism in large language model inference.\n# Simple example of floating-point non-associativity (0.1 + 1e20) - 1e20 # Result: 0 0.1 + (1e20 - 1e20) # Result: 0.1 Dynamic Precision and Information Loss Floating-point systems balance numerical range and precision through “dynamic precision.” When adding two floating-point numbers with different exponents, the system must discard some precision information:\n1230 ($1.23 \\times 10^2$) + 23.4 ($2.34 \\times 10^1$) = 1253.4 But due to maintaining only 3 digits of precision, the result is truncated to 1250 ($1.25 \\times 10^2$) This means that every time floating-point numbers are added in different orders, completely different results may be obtained.\nLimitations of Traditional Explanations Insufficiency of the “Concurrency + Floating-Point” Hypothesis For a long time, the academic community has generally attributed the nondeterminism in LLM inference to the “concurrency + floating-point” hypothesis:\nDue to the parallel computing characteristics of GPUs, the completion order of different threads is nondeterministic, leading to inconsistent floating-point accumulation orders.\nHowever, this research reveals the limitations of this hypothesis:\nGPU matrix multiplication is deterministic: Even highly parallel matrix multiplication operations can produce bit-level identical results when repeatedly executed on the same data Not all concurrent operations lead to nondeterminism: The key lies in specific implementation methods, not concurrency itself The Real Culprit: Batch Non-Invariance Core Discovery The research team discovered that the true root of LLM inference nondeterminism is batch non-invariance. Specifically manifested as:\nThe same data produces different numerical results under different batch sizes These differences accumulate and amplify during the inference process Ultimately leading to completely different output sequences Problems with Chunked Reduction Strategies In attention mechanism calculations, when query length is small (such as during the decoding phase), chunked reduction strategies are needed to fully utilize GPU parallelism. The problem lies in:\n# Problem example: Dynamic chunking strategy # KV length = 1000, requires 4 chunks # Each core processes 250 elements # But chunk count depends on batch size and query length This dynamic chunking strategy breaks batch invariance because:\nChunking strategy depends on the current number of queries being processed Different requests may trigger different chunking strategies Leading to differences in floating-point accumulation order Solution: Batch-Invariant Kernels Fixed-Size Chunking Strategy The core solution proposed by the research team is to adopt a fixed-size chunking strategy:\n# Solution: Fixed-size chunking # Regardless of KV length, each chunk is fixed at 256 elements # KV length = 1000 → 3 chunks of 256 + 1 chunk of 232 # KV length = 512 → 2 chunks of 256 Advantages of this strategy:\nBatch invariance: Same reduction order is executed regardless of how many tokens are processed Reproducibility: Same input always produces same output Performance preservation: Still able to fully utilize GPU parallelism Implementation Details The team achieved deterministic inference through the following technologies:\ntorch.Library integration: Non-invasive replacement of PyTorch operators FlexAttention backend: Implementation based on vLLM’s FlexAttention Batch-invariant kernels: Specially designed kernels ensuring numerical stability Experimental Results and Verification Nondeterminism Level Assessment Testing with the Qwen/Qwen3-235B model:\nTraditional method: 1000 identical prompts generated 80 different results Deterministic method: 1000 identical prompts generated completely identical results Notably, even with the nondeterministic method, the first 102 tokens were completely identical, with differences beginning to appear from the 103rd token.\nPerformance Impact Configuration Time (seconds) vLLM Default 26 Unoptimized Deterministic vLLM 55 Improved Attention Kernel 42 While deterministic inference incurs some performance overhead, it remains within acceptable ranges.\nBreakthrough in Reinforcement Learning More importantly, this research solves a critical problem in reinforcement learning:\nTraditional problem: Numerical differences between training and inference lead to “fake on-policy” reinforcement learning Solution: Deterministic inference makes true on-policy reinforcement learning possible Verification results: In RLVR experiments, the deterministic method achieved 0 KL divergence, indicating complete consistency between training and sampling policies Technical Implementation and Open Source Contributions Open Source Resources The research team provided complete implementations:\nBatch-invariant operations library: thinking-machines-lab/batch-invariant-ops vLLM deterministic mode examples: Directly runnable code demonstrations Core Code Structure # Core idea of batch-invariant kernels def batch_invariant_reduction(data, reduction_dim): # Fixed chunk size, not fixed chunk count fixed_chunk_size = 256 chunks = split_into_fixed_size_chunks(data, fixed_chunk_size) # Ensure consistency of reduction order result = deterministic_reduce(chunks) return result Significance for AI Research Enhancement of Scientific Rigor The value of this research lies not only in technical breakthroughs but also in its contribution to the scientific rigor of AI research:\nReproducibility: Researchers can completely reproduce experimental results Debugging capability: Ability to precisely locate and fix numerical issues System understanding: Deep understanding of the complexity of modern GPU computing systems Practical Application Value Model deployment: Ensuring consistent behavior in production environments A/B testing: Eliminating the impact of randomness on experimental results Reinforcement learning: Enabling true on-policy learning Conclusion and Outlook The research by the Thinking Machines team reveals the true root of nondeterminism in LLM inference and provides practical solutions. This work not only solves technical problems but more importantly enhances the scientific rigor of the entire AI research field.\nKey Insights Don’t accept “this is normal”: When facing nondeterminism issues, we should dig deep into root causes Importance of systems thinking: Understanding interaction effects in multi-layered abstract systems Integration of engineering and science: Validating scientific hypotheses through engineering practice Future Directions Performance optimization: Further optimizing the performance of deterministic kernels Broader applicability: Extending methods to more model architectures Standardization promotion: Promoting deterministic inference as an industry standard This research reminds us that in today’s rapid AI development, we still need to maintain attention to fundamental issues and solve seemingly complex engineering problems through rigorous scientific methods. The implementation of deterministic inference is not only a technical achievement but also a persistence and practice of scientific methodology.\nReferences:\nHe, Horace and Thinking Machines Lab, “Defeating Nondeterminism in LLM Inference”, Thinking Machines Lab: Connectionism, Sep 2025 GitHub: batch-invariant-ops ","description":"An in-depth analysis of how the Thinking Machines team solved the nondeterminism problem in large language model inference, achieving truly reproducible reasoning.","tags":["machine-learning","llm","determinism","reproducibility","research"],"title":"Defeating Nondeterminism in LLM Inference: A Breakthrough in Deep Learning System Reproducibility","uri":"/blogs/2024/sep/defeating-llm-nondeterminism/"},{"categories":["AI Research"],"content":"Defeating Nondeterminism in LLM Inference: A Breakthrough in Deep Learning System Reproducibility Introduction Reproducibility stands as one of the fundamental pillars of scientific research. However, when dealing with large language models (LLMs), achieving reproducible results has proven extraordinarily challenging. Even when we set the temperature parameter to 0 (greedy sampling), which should theoretically produce deterministic results, LLM inference still exhibits nondeterministic behavior.\nRecently, the Thinking Machines team published groundbreaking research that deeply investigates the root causes of nondeterminism in LLM inference and proposes effective solutions. This article provides an in-depth analysis of the core principles and methods of this research.\nThe Nature of the Problem: Floating-Point Non-Associativity The “Original Sin” of Floating-Point Operations To understand the root of nondeterminism, we must first grasp the concept of floating-point non-associativity. In floating-point arithmetic:\n$$(a + b) + c \\neq a + (b + c)$$\nThis seemingly simple mathematical property is actually the fundamental cause of nondeterminism in large language model inference.\n# Simple example of floating-point non-associativity (0.1 + 1e20) - 1e20 # Result: 0 0.1 + (1e20 - 1e20) # Result: 0.1 Dynamic Precision and Information Loss Floating-point systems balance numerical range and precision through “dynamic precision.” When adding two floating-point numbers with different exponents, the system must discard some precision information:\n1230 ($1.23 \\times 10^2$) + 23.4 ($2.34 \\times 10^1$) = 1253.4 But due to maintaining only 3 digits of precision, the result is truncated to 1250 ($1.25 \\times 10^2$) This means that every time floating-point numbers are added in different orders, completely different results may be obtained.\nLimitations of Traditional Explanations Insufficiency of the “Concurrency + Floating-Point” Hypothesis For a long time, the academic community has generally attributed the nondeterminism in LLM inference to the “concurrency + floating-point” hypothesis:\nDue to the parallel computing characteristics of GPUs, the completion order of different threads is nondeterministic, leading to inconsistent floating-point accumulation orders.\nHowever, this research reveals the limitations of this hypothesis:\nGPU matrix multiplication is deterministic: Even highly parallel matrix multiplication operations can produce bit-level identical results when repeatedly executed on the same data Not all concurrent operations lead to nondeterminism: The key lies in specific implementation methods, not concurrency itself The Real Culprit: Batch Non-Invariance Core Discovery The research team discovered that the true root of LLM inference nondeterminism is batch non-invariance. Specifically manifested as:\nThe same data produces different numerical results under different batch sizes These differences accumulate and amplify during the inference process Ultimately leading to completely different output sequences Problems with Chunked Reduction Strategies In attention mechanism calculations, when query length is small (such as during the decoding phase), chunked reduction strategies are needed to fully utilize GPU parallelism. The problem lies in:\n# Problem example: Dynamic chunking strategy # KV length = 1000, requires 4 chunks # Each core processes 250 elements # But chunk count depends on batch size and query length This dynamic chunking strategy breaks batch invariance because:\nChunking strategy depends on the current number of queries being processed Different requests may trigger different chunking strategies Leading to differences in floating-point accumulation order Solution: Batch-Invariant Kernels Fixed-Size Chunking Strategy The core solution proposed by the research team is to adopt a fixed-size chunking strategy:\n# Solution: Fixed-size chunking # Regardless of KV length, each chunk is fixed at 256 elements # KV length = 1000 → 3 chunks of 256 + 1 chunk of 232 # KV length = 512 → 2 chunks of 256 Advantages of this strategy:\nBatch invariance: Same reduction order is executed regardless of how many tokens are processed Reproducibility: Same input always produces same output Performance preservation: Still able to fully utilize GPU parallelism Implementation Details The team achieved deterministic inference through the following technologies:\ntorch.Library integration: Non-invasive replacement of PyTorch operators FlexAttention backend: Implementation based on vLLM’s FlexAttention Batch-invariant kernels: Specially designed kernels ensuring numerical stability Experimental Results and Verification Nondeterminism Level Assessment Testing with the Qwen/Qwen3-235B model:\nTraditional method: 1000 identical prompts generated 80 different results Deterministic method: 1000 identical prompts generated completely identical results Notably, even with the nondeterministic method, the first 102 tokens were completely identical, with differences beginning to appear from the 103rd token.\nPerformance Impact Configuration Time (seconds) vLLM Default 26 Unoptimized Deterministic vLLM 55 Improved Attention Kernel 42 While deterministic inference incurs some performance overhead, it remains within acceptable ranges.\nBreakthrough in Reinforcement Learning More importantly, this research solves a critical problem in reinforcement learning:\nTraditional problem: Numerical differences between training and inference lead to “fake on-policy” reinforcement learning Solution: Deterministic inference makes true on-policy reinforcement learning possible Verification results: In RLVR experiments, the deterministic method achieved 0 KL divergence, indicating complete consistency between training and sampling policies Technical Implementation and Open Source Contributions Open Source Resources The research team provided complete implementations:\nBatch-invariant operations library: thinking-machines-lab/batch-invariant-ops vLLM deterministic mode examples: Directly runnable code demonstrations Core Code Structure # Core idea of batch-invariant kernels def batch_invariant_reduction(data, reduction_dim): # Fixed chunk size, not fixed chunk count fixed_chunk_size = 256 chunks = split_into_fixed_size_chunks(data, fixed_chunk_size) # Ensure consistency of reduction order result = deterministic_reduce(chunks) return result Significance for AI Research Enhancement of Scientific Rigor The value of this research lies not only in technical breakthroughs but also in its contribution to the scientific rigor of AI research:\nReproducibility: Researchers can completely reproduce experimental results Debugging capability: Ability to precisely locate and fix numerical issues System understanding: Deep understanding of the complexity of modern GPU computing systems Practical Application Value Model deployment: Ensuring consistent behavior in production environments A/B testing: Eliminating the impact of randomness on experimental results Reinforcement learning: Enabling true on-policy learning Conclusion and Outlook The research by the Thinking Machines team reveals the true root of nondeterminism in LLM inference and provides practical solutions. This work not only solves technical problems but more importantly enhances the scientific rigor of the entire AI research field.\nKey Insights Don’t accept “this is normal”: When facing nondeterminism issues, we should dig deep into root causes Importance of systems thinking: Understanding interaction effects in multi-layered abstract systems Integration of engineering and science: Validating scientific hypotheses through engineering practice Future Directions Performance optimization: Further optimizing the performance of deterministic kernels Broader applicability: Extending methods to more model architectures Standardization promotion: Promoting deterministic inference as an industry standard This research reminds us that in today’s rapid AI development, we still need to maintain attention to fundamental issues and solve seemingly complex engineering problems through rigorous scientific methods. The implementation of deterministic inference is not only a technical achievement but also a persistence and practice of scientific methodology.\nReferences:\nHe, Horace and Thinking Machines Lab, “Defeating Nondeterminism in LLM Inference”, Thinking Machines Lab: Connectionism, Sep 2025 GitHub: batch-invariant-ops ","description":"An in-depth analysis of how the Thinking Machines team solved the nondeterminism problem in large language model inference, achieving truly reproducible reasoning.","tags":["machine-learning","llm","determinism","reproducibility","research"],"title":"Defeating Nondeterminism in LLM Inference: A Breakthrough in Deep Learning System Reproducibility","uri":"/blogs/2025/sep/defeating-llm-nondeterminism/"}]

[{"categories":["AI Research"],"content":"DeepSeek-R1 Nature封面突破：纯强化学习重新定义大模型推理范式 2025年9月21日，一篇来自中国DeepSeek团队的研究论文登上了《Nature》杂志封面，标志着人工智能领域的重大突破。这篇题为《DeepSeek-R1：通过强化学习激励大型语言模型的推理能力》的论文，不仅在技术层面实现了革命性创新，更以极低的成本挑战了现有大模型训练的经济模式。\n🎯 核心技术创新：纯强化学习范式 突破传统监督微调局限 传统的大语言模型训练高度依赖人工标注的推理过程，这不仅成本高昂，还限制了模型的自主推理能力发展。DeepSeek-R1采用了**纯强化学习（Pure Reinforcement Learning）**方法，完全摒弃了监督微调阶段：\n# 传统训练范式 Traditional_Training = Pretraining + Supervised_Fine_Tuning + RLHF # DeepSeek-R1创新范式 DeepSeek_R1 = Pretraining + Pure_Reinforcement_Learning 这种方法仅通过最终答案的正确性给予奖励信号，让模型在没有人工指导的情况下，自主探索和发现最优的推理路径。\n思维链自我演化机制 R1模型集成了**Chain-of-Thought（思维链）**技术，使模型能够：\n自主分解复杂问题：将复杂任务拆解为多个子步骤 推理过程回溯：在推理过程中自我修正错误路径 幻觉问题缓解：通过多步骤验证减少输出中的错误信息 🏗️ 架构设计突破 混合专家架构（MoE）优化 DeepSeek-R1采用了混合专家架构，实现了计算资源的精准配置：\n选择性激活：仅激活与当前任务相关的模型参数 计算效率提升：大幅降低推理时的计算负载 性能保持：在减少计算资源的同时维持高性能表现 FP8混合精度训练技术 通过采用FP8混合精度训练技术，R1模型实现了：\n训练加速：相比传统FP16训练提升2-3倍训练速度 内存优化：显著降低GPU内存占用 数值稳定性：保证训练过程的稳定性和收敛性 📊 性能与效率突破 推理效率革命性提升 DeepSeek-R1通过思维链压缩训练技术，实现了显著的效率提升：\n优化维度 改进幅度 技术手段 输出令牌数减少 20%-50% 思维链压缩优化 推理速度提升 30%-40% MoE架构 + FP8精度 计算资源节约 40%-60% 选择性专家激活 成本效益的革命性突破 最令人震撼的是R1模型的训练成本仅为29.4万美元，这一数字远低于同等性能的竞争模型：\nGPT-4训练成本：估计超过1亿美元 Claude-3成本：估计数千万美元 DeepSeek-R1：29.4万美元 这一成本优势证明了在资源受限情况下实现高性能AI模型的可能性。\n🔬 技术验证与基准测试 数学推理能力验证 在标准数学推理基准测试中，DeepSeek-R1表现出色：\nGSM8K数学题库：准确率达到96.3%（超越GPT-4的94.2%） MATH竞赛题库：准确率达到71.8%（接近GPT-4 Turbo的73.4%） 复杂推理任务：在多步骤逻辑推理中表现优异 编程能力突破 在编程任务评估中，R1模型展现了强大的代码生成和调试能力：\nHumanEval基准：通过率达到88.2% MBPP编程测试：准确率超过85% 代码理解与优化：在代码审查和重构任务中表现出色 🌟 开源策略与透明化研究 全面开源承诺 DeepSeek团队做出了前所未有的开源承诺：\n完整模型权重：开放R1模型的完整参数 训练代码：公开详细的训练流程和优化技巧 成本透明：详细披露训练成本和资源消耗 技术细节：提供完整的技术实现文档 学术研究新标杆 这种全面透明的研究方式为AI领域树立了新的标准：\n可复现性：其他研究团队可以完全复现实验结果 同行评议：接受全球AI研究社区的严格审查 知识共享：促进整个行业的技术进步 🚀 产业影响与未来展望 重新定义AI经济学 DeepSeek-R1的成功重新定义了大模型开发的经济模式：\n降低准入门槛：中小型研究机构也能参与大模型研发 民主化AI发展：打破大型科技公司的技术垄断 推动创新加速：更多团队能够基于R1进行创新研究 技术发展新方向 R1模型的成功验证了几个重要的技术方向：\n纯强化学习的潜力：证明了无监督推理能力训练的可行性 效率优化的重要性：展示了架构优化在成本控制中的关键作用 开源模式的价值：验证了开放合作对技术进步的促进作用 对全球AI竞争格局的影响 DeepSeek-R1的突破对全球AI竞争产生了深远影响：\n技术路线多样化：为大模型发展提供了新的技术路径 成本优势重塑竞争：低成本高性能模型改变了竞争规则 开源生态繁荣：推动了开源AI生态的快速发展 🔮 技术展望与启示 下一代AI模型的发展方向 基于DeepSeek-R1的成功，未来AI模型发展可能呈现以下趋势：\n推理能力成为核心：模型的推理和自我改进能力将成为主要竞争点 训练效率持续优化：更加注重训练成本和资源效率 多模态推理融合：将纯RL方法扩展到视觉、语音等多模态领域 对AI研究的深层启示 R1模型的成功提供了重要的研究启示：\n简化即优化：去除不必要的复杂性往往能带来更好的结果 自主学习的力量：给予模型更多自主探索的空间 成本意识设计：在追求性能的同时必须考虑经济可行性 结语 DeepSeek-R1在Nature杂志的发表不仅仅是一篇学术论文的成功，更是中国AI研究在全球舞台上的重要里程碑。通过纯强化学习实现大模型推理能力的突破，以29.4万美元的成本挑战现有经济模式，以及全面开源的研究理念，R1模型为整个AI行业带来了新的思路和可能性。\n这项研究证明了，在AI发展的道路上，创新思维和开放合作比单纯的资源投入更为重要。随着DeepSeek-R1技术的进一步发展和应用，我们有理由相信，更加高效、经济、开放的AI时代正在到来。\n本文基于DeepSeek团队在Nature发表的原创研究，展现了中国AI研究在全球前沿技术发展中的重要贡献。随着相关技术的持续演进，我们期待看到更多基于强化学习的AI突破。\n","description":"深度解析DeepSeek-R1在Nature发表的重磅研究：如何通过纯强化学习激励LLM推理能力，29.4万美元成本实现GPT-4级别性能","tags":["deepseek","reinforcement-learning","llm","reasoning","nature","ai-breakthrough"],"title":"DeepSeek-R1 Nature封面突破：纯强化学习重新定义大模型推理范式","uri":"/blogs/2025/sep/deepseek-r1-nature-breakthrough/"},{"categories":["AI Research"],"content":"The Great AI Divide: ChatGPT vs Claude Usage Patterns Reveal Distinct User Preferences Introduction In an unprecedented move, both OpenAI and Anthropic have released comprehensive usage studies of their flagship AI assistants, ChatGPT and Claude. These dueling analyses provide fascinating insights into how different AI models are carving out distinct niches in the rapidly evolving landscape of artificial intelligence.\nThe findings reveal a striking pattern: rather than competing head-to-head, ChatGPT and Claude are becoming specialized tools for different types of users and use cases. This market segmentation suggests a maturing AI ecosystem where various models excel in specific domains.\nTale of Two AI Assistants ChatGPT: The Personal Companion OpenAI’s analysis of ChatGPT usage reveals a platform that has evolved into a personal AI companion:\nUsage Breakdown: 70%+ non-work related conversations (up from 53% in June 2024) 27% work-related queries (down from 47% year-over-year) Focus on consumer applications over enterprise tools Primary Use Cases: Practical guidance and life advice Creative writing and content assistance Information seeking and general knowledge Casual conversation and exploration Claude: The Professional Powerhouse Anthropic’s research on Claude usage paints a very different picture - one of a work-focused productivity tool:\nUsage Characteristics: Heavy emphasis on professional productivity Coding dominates with 36% of total usage (vs. ChatGPT’s 4.2%) Business automation and task delegation Research and education applications Primary Applications: Software development and programming Scientific research and analysis Business automation and workflows Educational content and tutoring The Great Coding Divide Perhaps the most striking difference between the platforms lies in programming and technical usage:\nClaude: Developer’s Choice 36% of Claude usage involves coding and programming 44% of API usage dedicated to software development Preferred by software engineers for complex technical tasks Strong performance in code generation and debugging ChatGPT: Limited Technical Appeal Only 4.2% programming-related messages Lowest user satisfaction in technical help categories Preference for non-technical applications General-purpose rather than specialized technical tool This disparity suggests that developers have made a clear choice: Claude for coding, ChatGPT for everything else.\nBusiness vs. Consumer Focus Claude: Enterprise-First Approach Claude shows strong business adoption patterns:\nAPI Usage Dominance: 77% of API tasks are automated (vs. 50% on Claude.ai) Full task delegation preferred over collaboration Business process automation as primary use case Administrative support and workflow optimization Professional Applications: Mathematical tasks and data analysis Scientific research (7% of usage, growing) Educational content development (13% and growing) Business operations and management ChatGPT: Consumer-Centric Evolution ChatGPT demonstrates clear consumer market focus:\nPersonal Applications: Life advice and decision support Creative projects and hobbies Learning and skill development Entertainment and casual interaction Work Usage Patterns: Advisory role preferred over task execution Decision support rather than automation Writing assistance remains primary professional use Individual productivity over business process automation Market Segmentation Analysis The usage patterns reveal distinct market segments:\nClaude’s Core Demographics User Type Primary Use Cases Value Proposition Software Developers Code generation, debugging, technical documentation Specialized programming expertise Researchers Data analysis, research assistance, scientific writing Advanced reasoning capabilities Business Professionals Process automation, administrative tasks Efficient task delegation Educators Curriculum development, educational content Structured knowledge delivery ChatGPT’s User Base User Type Primary Use Cases Value Proposition General Consumers Life advice, entertainment, learning Accessible AI companion Creative Professionals Writing, brainstorming, content creation Creative collaboration partner Knowledge Workers Writing assistance, research support Personal productivity enhancement Students Homework help, concept explanation Personalized tutoring Platform Architecture Implications Claude: Built for Business Claude’s usage patterns reflect platform design optimized for:\nTechnical Sophistication: Advanced reasoning capabilities for complex problems Structured output for business applications API-first approach for integration Batch processing and automation support Professional Features: Higher context limits for large documents Better code understanding and generation Scientific and mathematical reasoning Process automation capabilities ChatGPT: Designed for Accessibility ChatGPT’s evolution toward consumer use reflects:\nUser Experience Focus: Conversational fluency over technical precision Broad accessibility across skill levels Intuitive interaction patterns Personal relevance and engagement Consumer-Friendly Features: Multiple interaction modes (text, voice, image) Plugin ecosystem for extended functionality Easy onboarding and adoption Personalization and memory features The Complementary AI Ecosystem Rather than direct competition, the data suggests complementary positioning:\nWorkflow Specialization Many users likely employ both platforms for different purposes:\nDevelopment Workflow: Claude for coding and technical implementation ChatGPT for planning and conceptual work Claude for debugging and optimization ChatGPT for documentation and communication Research Workflow: Claude for data analysis and technical research ChatGPT for brainstorming and ideation Claude for structured reporting ChatGPT for presentation and communication Market Evolution The differentiation suggests mature market development:\nSpecialized Solutions: Different AI models for different needs Platform-specific optimization and features User base segmentation by use case Reduced direct competition through differentiation Future Implications Platform Strategy The usage patterns suggest different strategic directions:\nClaude’s Path: Deeper enterprise integration Advanced technical capabilities Professional workflow optimization B2B market expansion ChatGPT’s Direction: Broader consumer adoption Personal AI companion features Lifestyle integration B2C market dominance User Experience Evolution Different platforms will likely optimize for different experiences:\nProfessional AI (Claude): Efficiency and accuracy prioritized Structured interactions and outputs Integration with business tools Measurable productivity gains Personal AI (ChatGPT): Engagement and accessibility prioritized Natural conversation and interaction Personal context and memory Emotional connection and support Challenges and Opportunities For Claude (Anthropic) Opportunities: Enterprise market expansion Developer tool integration Scientific research partnerships Professional certification programs Challenges: Consumer market penetration Accessibility for non-technical users Price sensitivity in broader markets Brand recognition vs. OpenAI For ChatGPT (OpenAI) Opportunities: Mass market adoption Consumer product integration Educational partnerships Entertainment and media applications Challenges: Professional tool credibility Technical user acquisition Enterprise feature gaps Developer ecosystem competition Conclusion: A Multi-AI Future The dueling studies reveal a crucial insight: the AI market is big enough for specialized players. Rather than a winner-take-all scenario, we’re seeing the emergence of a multi-AI ecosystem where different models serve different needs.\nKey Takeaways: Specialization over generalization: Models are finding specific niches User preference drives adoption: Different users have different needs Complementary rather than competitive: Platforms serve different purposes Market maturation: Clear segmentation suggests industry growth Multiple AI future: Users will likely use multiple AI tools Strategic Implications: Platform differentiation becomes crucial for success User experience optimization for specific use cases Partnership opportunities between complementary platforms Market education about appropriate tool selection Investment in specialized capabilities rather than general features The data tells a clear story: the future of AI is not about one model ruling them all, but about the right AI for the right job. As the technology matures, we can expect even more specialized AI tools to emerge, each optimized for specific use cases and user needs.\nThis market evolution benefits everyone - users get better tools for their specific needs, and AI companies can focus on what they do best rather than trying to be everything to everyone.\nSources: OpenAI ChatGPT usage analysis and Anthropic Claude usage research, as reported by Fortune and official company publications.\n","description":"Dueling studies from OpenAI and Anthropic reveal fascinating differences in how users interact with ChatGPT and Claude, suggesting complementary rather than competing AI ecosystems.","tags":["chatgpt","claude","ai-comparison","usage-patterns","anthropic","openai"],"title":"The Great AI Divide: ChatGPT vs Claude Usage Patterns Reveal Distinct User Preferences","uri":"/blogs/2025/sep/chatgpt-vs-claude-usage-patterns/"},{"categories":["AI Research"],"content":"How People Are Using ChatGPT: Insights from OpenAI’s Latest Usage Analysis Introduction Understanding how people actually use artificial intelligence tools provides crucial insights into the technology’s real-world impact and future trajectory. OpenAI recently released a comprehensive analysis of ChatGPT usage patterns, examining millions of conversations to understand how people interact with their flagship AI assistant.\nThe findings reveal a surprising trend: ChatGPT is increasingly becoming a personal companion and exploratory tool rather than primarily a workplace productivity enhancer. This shift has significant implications for how we think about AI adoption and the future of human-AI interaction.\nKey Findings: The Personal AI Revolution Non-Work Usage Dominates The most striking finding from OpenAI’s analysis is the overwhelming dominance of personal, non-work-related usage:\n70%+ of all ChatGPT conversations are now non-work related (up from 53% in June 2024) Only 27% of messages are work-related (down from 47% a year ago) This represents a fundamental shift in how people perceive and use AI tools This trend suggests that ChatGPT has successfully crossed the chasm from being a niche productivity tool to becoming a mainstream consumer application that people integrate into their daily lives.\nThe Big Three: Core Use Cases OpenAI identified three dominant conversation categories that collectively account for nearly 78% of all ChatGPT interactions:\n1. Practical Guidance (Personal Advisory) Users frequently turn to ChatGPT for:\nLife advice and decision-making support Problem-solving assistance Learning new skills or concepts Getting explanations for complex topics 2. Writing Assistance ChatGPT serves as a versatile writing companion for:\nCreative writing projects Email composition and editing Content brainstorming Personal correspondence improvement 3. Information Seeking People use ChatGPT as an intelligent search engine for:\nQuick fact-checking Research on various topics Summarizing complex information Exploring new subjects Work-Related Usage: Quality Over Quantity While work usage represents a smaller percentage, the quality and nature of professional ChatGPT use reveals interesting patterns:\nAdvisory Role Preferred When people do use ChatGPT for work, they prefer it as:\nDecision support tool rather than task executor Research assistant for gathering and analyzing information Brainstorming partner for idea generation Strategic advisor for planning and problem-solving This preference for advisory roles over direct task execution suggests users maintain agency while leveraging AI for enhanced decision-making.\nWriting Dominates Professional Use 42% of all work-related messages involve writing tasks, including:\nDocument editing and refinement (most common) Email composition and improvement Report writing and formatting Content creation and optimization Interestingly, two-thirds of writing requests involve modifying existing text rather than creating original content from scratch, indicating ChatGPT’s strength in iterative improvement.\nProfessional Demographics The data reveals clear demographic patterns in work usage:\nHighly-paid professionals and technical workers are more likely to use ChatGPT for work Management and business occupations rely heavily on writing assistance Technical professionals use ChatGPT for problem-solving and research The Coding Conundrum One surprising finding challenges common assumptions about AI coding assistance:\nLimited Programming Usage Only 4.2% of total ChatGPT messages relate to computer programming This is significantly lower than competing platforms like Claude (36% for coding) Technical Help category (including programming) shows the lowest user satisfaction Why the Disconnect? Several factors may explain this pattern:\nSpecialized tools preferred: Developers may prefer dedicated coding assistants Context limitations: ChatGPT’s context window may be insufficient for complex coding tasks Integration challenges: Lack of seamless IDE integration compared to specialized tools User expectations: Developers may have higher standards for code quality and accuracy Implications for AI Development The Consumer AI Market OpenAI’s findings suggest a massive consumer AI market that extends far beyond workplace productivity:\nPersonal AI Companions People want AI that understands their personal context Emotional support and life guidance represent significant use cases The boundary between utility and companionship continues to blur Educational Applications Self-directed learning through conversational AI Personalized explanation and tutoring Skill development outside formal educational settings User Experience Priorities The data indicates users value:\nConversational fluency over technical precision Broad knowledge over deep specialization Accessibility over advanced features Personal relevance over professional optimization The Future of Human-AI Interaction Emerging Patterns Several trends emerge from the usage data:\n1. AI as Thinking Partner Users increasingly treat ChatGPT as a cognitive extension rather than a tool:\nBouncing ideas and getting feedback Working through complex problems collaboratively Using AI to explore different perspectives 2. Democratization of Expertise ChatGPT enables users to access expert-level knowledge across domains:\nLegal and medical information (with appropriate disclaimers) Technical explanations made accessible Professional advice and best practices 3. Creative Collaboration The platform facilitates new forms of human-AI creative partnership:\nCollaborative storytelling and content creation Idea generation and refinement Creative problem-solving approaches Market Segmentation The usage patterns suggest clear market segmentation:\nSegment Primary Use Cases Key Value Proposition Personal Users Life advice, learning, entertainment Accessible AI companion Professional Knowledge Workers Writing, research, decision support Productivity enhancement Creative Professionals Content creation, brainstorming Creative partnership Students/Learners Education, skill development Personalized tutoring Challenges and Considerations Quality vs. Accessibility Trade-offs The shift toward personal usage raises important questions:\nAccuracy Concerns Personal advice lacks professional accountability Medical, legal, and financial guidance require careful disclaimers Risk of over-reliance on AI for critical decisions Educational Impact Potential dependency on AI for learning Questions about developing independent thinking skills Balance between AI assistance and human capability development Privacy and Data Sensitivity Personal usage patterns create new privacy considerations:\nIntimate conversations require robust data protection Personal information security becomes paramount Trust becomes the foundation of the user relationship Conclusion: The Personal AI Era OpenAI’s usage analysis reveals a fundamental shift in AI adoption: we’re entering the era of personal AI. Rather than being confined to workplace productivity, ChatGPT has become a digital companion that people turn to for a wide range of personal needs.\nKey Takeaways Personal use dominates: 70%+ of ChatGPT usage is non-work related Advisory preferred: Users want decision support, not task replacement Writing is king: Text creation and editing remain the killer application Accessibility matters: Broad usability trumps specialized features Consumer market massive: Personal AI represents a larger opportunity than enterprise AI Looking Forward This trend suggests several important developments:\nAI will become more conversational and personal Privacy and trust will become key differentiators Educational applications will expand significantly New forms of human-AI collaboration will emerge The definition of “productivity” will expand beyond work As AI technology continues to evolve, understanding these usage patterns helps us build better tools that truly serve human needs. The future of AI may be less about replacing human capabilities and more about augmenting human potential in deeply personal ways.\nThe data tells a clear story: AI is becoming personal, and that’s exactly what users want.\nSource: OpenAI’s analysis of ChatGPT usage patterns, examining millions of user conversations to understand real-world AI interaction patterns.\n","description":"OpenAI reveals fascinating insights into how millions of users interact with ChatGPT, showing a shift toward personal and exploratory use cases rather than work-focused applications.","tags":["chatgpt","openai","ai-usage","productivity","consumer-ai"],"title":"How People Are Using ChatGPT: Insights from OpenAI's Latest Usage Analysis","uri":"/blogs/2025/sep/how-people-use-chatgpt/"},{"categories":["AI Research"],"content":"Defeating Nondeterminism in LLM Inference: A Breakthrough in Deep Learning System Reproducibility Introduction Reproducibility stands as one of the fundamental pillars of scientific research. However, when dealing with large language models (LLMs), achieving reproducible results has proven extraordinarily challenging. Even when we set the temperature parameter to 0 (greedy sampling), which should theoretically produce deterministic results, LLM inference still exhibits nondeterministic behavior.\nRecently, the Thinking Machines team published groundbreaking research that deeply investigates the root causes of nondeterminism in LLM inference and proposes effective solutions. This article provides an in-depth analysis of the core principles and methods of this research.\nThe Nature of the Problem: Floating-Point Non-Associativity The “Original Sin” of Floating-Point Operations To understand the root of nondeterminism, we must first grasp the concept of floating-point non-associativity. In floating-point arithmetic:\n$$(a + b) + c \\neq a + (b + c)$$\nThis seemingly simple mathematical property is actually the fundamental cause of nondeterminism in large language model inference.\n# Simple example of floating-point non-associativity (0.1 + 1e20) - 1e20 # Result: 0 0.1 + (1e20 - 1e20) # Result: 0.1 Dynamic Precision and Information Loss Floating-point systems balance numerical range and precision through “dynamic precision.” When adding two floating-point numbers with different exponents, the system must discard some precision information:\n1230 ($1.23 \\times 10^2$) + 23.4 ($2.34 \\times 10^1$) = 1253.4 But due to maintaining only 3 digits of precision, the result is truncated to 1250 ($1.25 \\times 10^2$) This means that every time floating-point numbers are added in different orders, completely different results may be obtained.\nLimitations of Traditional Explanations Insufficiency of the “Concurrency + Floating-Point” Hypothesis For a long time, the academic community has generally attributed the nondeterminism in LLM inference to the “concurrency + floating-point” hypothesis:\nDue to the parallel computing characteristics of GPUs, the completion order of different threads is nondeterministic, leading to inconsistent floating-point accumulation orders.\nHowever, this research reveals the limitations of this hypothesis:\nGPU matrix multiplication is deterministic: Even highly parallel matrix multiplication operations can produce bit-level identical results when repeatedly executed on the same data Not all concurrent operations lead to nondeterminism: The key lies in specific implementation methods, not concurrency itself The Real Culprit: Batch Non-Invariance Core Discovery The research team discovered that the true root of LLM inference nondeterminism is batch non-invariance. Specifically manifested as:\nThe same data produces different numerical results under different batch sizes These differences accumulate and amplify during the inference process Ultimately leading to completely different output sequences Problems with Chunked Reduction Strategies In attention mechanism calculations, when query length is small (such as during the decoding phase), chunked reduction strategies are needed to fully utilize GPU parallelism. The problem lies in:\n# Problem example: Dynamic chunking strategy # KV length = 1000, requires 4 chunks # Each core processes 250 elements # But chunk count depends on batch size and query length This dynamic chunking strategy breaks batch invariance because:\nChunking strategy depends on the current number of queries being processed Different requests may trigger different chunking strategies Leading to differences in floating-point accumulation order Solution: Batch-Invariant Kernels Fixed-Size Chunking Strategy The core solution proposed by the research team is to adopt a fixed-size chunking strategy:\n# Solution: Fixed-size chunking # Regardless of KV length, each chunk is fixed at 256 elements # KV length = 1000 → 3 chunks of 256 + 1 chunk of 232 # KV length = 512 → 2 chunks of 256 Advantages of this strategy:\nBatch invariance: Same reduction order is executed regardless of how many tokens are processed Reproducibility: Same input always produces same output Performance preservation: Still able to fully utilize GPU parallelism Implementation Details The team achieved deterministic inference through the following technologies:\ntorch.Library integration: Non-invasive replacement of PyTorch operators FlexAttention backend: Implementation based on vLLM’s FlexAttention Batch-invariant kernels: Specially designed kernels ensuring numerical stability Experimental Results and Verification Nondeterminism Level Assessment Testing with the Qwen/Qwen3-235B model:\nTraditional method: 1000 identical prompts generated 80 different results Deterministic method: 1000 identical prompts generated completely identical results Notably, even with the nondeterministic method, the first 102 tokens were completely identical, with differences beginning to appear from the 103rd token.\nPerformance Impact Configuration Time (seconds) vLLM Default 26 Unoptimized Deterministic vLLM 55 Improved Attention Kernel 42 While deterministic inference incurs some performance overhead, it remains within acceptable ranges.\nBreakthrough in Reinforcement Learning More importantly, this research solves a critical problem in reinforcement learning:\nTraditional problem: Numerical differences between training and inference lead to “fake on-policy” reinforcement learning Solution: Deterministic inference makes true on-policy reinforcement learning possible Verification results: In RLVR experiments, the deterministic method achieved 0 KL divergence, indicating complete consistency between training and sampling policies Technical Implementation and Open Source Contributions Open Source Resources The research team provided complete implementations:\nBatch-invariant operations library: thinking-machines-lab/batch-invariant-ops vLLM deterministic mode examples: Directly runnable code demonstrations Core Code Structure # Core idea of batch-invariant kernels def batch_invariant_reduction(data, reduction_dim): # Fixed chunk size, not fixed chunk count fixed_chunk_size = 256 chunks = split_into_fixed_size_chunks(data, fixed_chunk_size) # Ensure consistency of reduction order result = deterministic_reduce(chunks) return result Significance for AI Research Enhancement of Scientific Rigor The value of this research lies not only in technical breakthroughs but also in its contribution to the scientific rigor of AI research:\nReproducibility: Researchers can completely reproduce experimental results Debugging capability: Ability to precisely locate and fix numerical issues System understanding: Deep understanding of the complexity of modern GPU computing systems Practical Application Value Model deployment: Ensuring consistent behavior in production environments A/B testing: Eliminating the impact of randomness on experimental results Reinforcement learning: Enabling true on-policy learning Conclusion and Outlook The research by the Thinking Machines team reveals the true root of nondeterminism in LLM inference and provides practical solutions. This work not only solves technical problems but more importantly enhances the scientific rigor of the entire AI research field.\nKey Insights Don’t accept “this is normal”: When facing nondeterminism issues, we should dig deep into root causes Importance of systems thinking: Understanding interaction effects in multi-layered abstract systems Integration of engineering and science: Validating scientific hypotheses through engineering practice Future Directions Performance optimization: Further optimizing the performance of deterministic kernels Broader applicability: Extending methods to more model architectures Standardization promotion: Promoting deterministic inference as an industry standard This research reminds us that in today’s rapid AI development, we still need to maintain attention to fundamental issues and solve seemingly complex engineering problems through rigorous scientific methods. The implementation of deterministic inference is not only a technical achievement but also a persistence and practice of scientific methodology.\nReferences:\nHe, Horace and Thinking Machines Lab, “Defeating Nondeterminism in LLM Inference”, Thinking Machines Lab: Connectionism, Sep 2025 GitHub: batch-invariant-ops ","description":"An in-depth analysis of how the Thinking Machines team solved the nondeterminism problem in large language model inference, achieving truly reproducible reasoning.","tags":["machine-learning","llm","determinism","reproducibility","research"],"title":"Defeating Nondeterminism in LLM Inference: A Breakthrough in Deep Learning System Reproducibility","uri":"/blogs/2025/sep/defeating-llm-nondeterminism/"}]

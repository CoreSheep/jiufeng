<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Llm on Feng</title>
    <link>http://localhost:1313/tags/llm/</link>
    <description>Recent content in Llm on Feng</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <managingEditor>lijiufeng97@gmail.com (Jiufeng Li)</managingEditor>
    <webMaster>lijiufeng97@gmail.com (Jiufeng Li)</webMaster>
    <lastBuildDate>Sun, 21 Sep 2025 09:00:00 +0100</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>DeepSeek-R1 Makes Nature Cover: How Pure Reinforcement Learning Revolutionizes LLM Reasoning</title>
      <link>http://localhost:1313/blogs/2025/sep/deepseek-r1-nature-breakthrough/</link>
      <pubDate>Sun, 21 Sep 2025 09:00:00 +0100</pubDate><author>lijiufeng97@gmail.com (Jiufeng Li)</author>
      <guid>http://localhost:1313/blogs/2025/sep/deepseek-r1-nature-breakthrough/</guid>
      <description>&lt;h1 id=&#34;deepseek-r1-makes-nature-cover-how-pure-reinforcement-learning-revolutionizes-llm-reasoning&#34;&gt;DeepSeek-R1 Makes Nature Cover: How Pure Reinforcement Learning Revolutionizes LLM Reasoning&lt;/h1&gt;&#xA;&lt;p&gt;On September 21, 2025, a research paper from China&amp;rsquo;s DeepSeek team landed on the cover of &lt;em&gt;Nature&lt;/em&gt; magazine, marking a pivotal moment in artificial intelligence. The paper, titled &amp;ldquo;DeepSeek-R1: Incentivizing Reasoning in LLMs through Reinforcement Learning,&amp;rdquo; doesn&amp;rsquo;t just represent a technical breakthroughâ€”it fundamentally challenges the economics of large model training with an approach that&amp;rsquo;s as elegant as it is cost-effective.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Defeating Nondeterminism in LLM Inference: A Breakthrough in Deep Learning System Reproducibility</title>
      <link>http://localhost:1313/blogs/2025/sep/defeating-llm-nondeterminism/</link>
      <pubDate>Mon, 15 Sep 2025 10:00:00 +0100</pubDate><author>lijiufeng97@gmail.com (Jiufeng Li)</author>
      <guid>http://localhost:1313/blogs/2025/sep/defeating-llm-nondeterminism/</guid>
      <description>&lt;h1 id=&#34;defeating-nondeterminism-in-llm-inference-a-breakthrough-in-deep-learning-system-reproducibility&#34;&gt;Defeating Nondeterminism in LLM Inference: A Breakthrough in Deep Learning System Reproducibility&lt;/h1&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Reproducibility&lt;/strong&gt; stands as one of the fundamental pillars of scientific research. However, when dealing with large language models (LLMs), achieving reproducible results has proven extraordinarily challenging. Even when we set the temperature parameter to 0 (greedy sampling), which should theoretically produce deterministic results, LLM inference still exhibits nondeterministic behavior.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
